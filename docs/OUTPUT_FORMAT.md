# Weval Output JSON Format

This document details the structure of the JSON files generated by the `run-config` command. These files contain the complete results of an evaluation run and are the primary data source for the web dashboard and any downstream analysis.

The root of the JSON object corresponds to the `FinalComparisonOutputV2` type definition in the project's source code.

## 1. Top-Level Structure

The main JSON object has the following top-level properties:

| Key | Type | Description |
| --- | --- | --- |
| `configId` | `string` | The unique identifier for the blueprint, derived from the `id` field or filename. |
| `configTitle` | `string` | The human-readable title of the blueprint. |
| `runLabel` | `string` | The unique label for this execution, combining the user-provided label and a content hash. |
| `timestamp` | `string` | An ISO 8601 timestamp indicating when the evaluation run was completed. |
| `description` | `string` (optional) | The description from the blueprint file. |
| `sourceCommitSha` | `string` (optional) | The Git commit SHA of the blueprint version used, if available. |
| `sourceBlueprintFileName` | `string` (optional) | The original filename of the blueprint if fetched remotely. |
| `config` | `object` | The full blueprint configuration object used for this run. See [Blueprint Format](BLUEPRINT_FORMAT.md). |
| `evalMethodsUsed` | `string[]` | An array of evaluation methods used (e.g., `["embedding", "llm-coverage"]`). |
| `effectiveModels` | `string[]` | An array of all model IDs that were run, including temperature or system prompt permutations (e.g., `openai:gpt-4o-mini[temp:0.7]`). |
| `modelSystemPrompts`| `object` (optional) | A map from an `effectiveModel` ID to the system prompt string used for it. |
| `promptIds` | `string[]` | An array of all prompt IDs evaluated. |
| `promptContexts` | `object` | A map from `promptId` to the full input prompt, which can be a single string or an array of `ConversationMessage` objects for multi-turn conversations. |
| `allFinalAssistantResponses` | `object` (optional) | A nested map: `promptId` -> `modelId` -> `final assistant response text`. |
| `fullConversationHistories` | `object` (optional) | A nested map: `promptId` -> `modelId` -> `ConversationMessage[]` representing the full exchange. |
| `errors` | `object` (optional) | A nested map for any errors encountered: `promptId` -> `modelId` -> `error message`. |
| `extractedKeyPoints`| `Record<string, string[]>` (optional) | A map from `promptId` to the key points extracted by the judge model from the `idealResponse`. |
| `evaluationResults`| `object` | An object containing the results from all evaluation methods. See [Evaluation Results Structure](#2-evaluation-results-structure). |

### `ConversationMessage` Object

This object is used in `promptContexts` and `fullConversationHistories`.

```typescript
interface ConversationMessage {
  role: 'user' | 'assistant' | 'system';
  content: string;
}
```

---

## 2. Evaluation Results Structure

The `evaluationResults` object contains the outputs from the different evaluation methods.

| Key | Type | Description |
| --- | --- | --- |
| `similarityMatrix` | `SimilarityScore` (optional) | A matrix of pairwise semantic similarity scores, averaged across all prompts. Only present if `embedding` method is used. |
| `perPromptSimilarities` | `Record<string, SimilarityScore>` (optional) | A map from `promptId` to its individual `SimilarityScore` matrix. |
| `llmCoverageScores` | `LLMCoverageScores` (optional) | Contains rubric-based evaluation scores. Only present if `llm-coverage` method is used. |

### `SimilarityScore` Object

This is a nested object representing a pairwise similarity matrix.

```typescript
// Example for { modelA, modelB, modelC }
{
  "modelA": { "modelB": 0.85, "modelC": 0.62 },
  "modelB": { "modelA": 0.85, "modelC": 0.71 },
  "modelC": { "modelA": 0.62, "modelB": 0.71 }
}
```

### `LLMCoverageScores` Object

This is a nested map containing the results of the rubric-based (`llm-coverage`) evaluation.
`LLMCoverageScores`: `promptId` -> `modelId` -> `CoverageResult`

A `CoverageResult` object can either be a successful result or an error object.

#### Successful `CoverageResult`

```typescript
{
  "keyPointsCount": number, // Total number of rubric items for this prompt
  "avgCoverageExtent": number, // The final score (0 to 1), averaged across all points, respecting multipliers
  "pointAssessments": PointAssessment[] // An array of assessments, one for each rubric item
}
```

#### `PointAssessment` Object

This object contains the detailed evaluation for a single rubric item.

```typescript
{
  "keyPointText": string, // The original text of the rubric item
  "coverageExtent": number, // The score for this specific point (0 to 1)
  "reflection": string, // The judge model's reasoning for its score
  "error": string, // Present if an error occurred for this point
  "multiplier": number, // The weight of this point
  "citation": string, // Optional citation associated with the point
  "judgeModelId": string, // The final judge model that produced this assessment
  "isInverted": boolean, // True if the point came from a `should_not` block
  "judgeLog": string[], // (Debug) Log of steps taken by the judge logic
  "individualJudgements": object[] // (Debug) The raw outputs from each judge in a consensus call
}
``` 