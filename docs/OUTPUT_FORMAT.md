# Weval Output JSON Format

This document details the structure of the JSON files generated by the `run-config` command. These files contain the complete results of an evaluation run and are the primary data source for the web dashboard and any downstream analysis.

The root of the JSON object corresponds to the `FinalComparisonOutputV2` type definition in the project's source code.

## 1. Top-Level Structure

The main JSON object has the following top-level properties:

| Key | Type | Description |
| --- | --- | --- |
| `configId` | `string` | The unique identifier for the blueprint, derived from the `id` field or filename. |
| `configTitle` | `string` | The human-readable title of the blueprint. |
| `runLabel` | `string` | The unique label for this execution, combining the user-provided label and a content hash. |
| `timestamp` | `string` | An ISO 8601 timestamp indicating when the evaluation run was completed. |
| `description` | `string` (optional) | The description from the blueprint file. |
| `sourceCommitSha` | `string` (optional) | The Git commit SHA of the blueprint version used, if available. |
| `sourceBlueprintFileName` | `string` (optional) | The original filename of the blueprint if fetched remotely. |
| `config` | `object` | The full blueprint configuration object used for this run. See [Blueprint Format](BLUEPRINT_FORMAT.md). |
| `evalMethodsUsed` | `string[]` | An array of evaluation methods used (e.g., `["embedding", "llm-coverage"]`). |
| `effectiveModels` | `string[]` | An array of all model IDs that were run, including temperature or system prompt permutations (e.g., `openai:gpt-4o-mini[temp:0.7]`). |
| `modelSystemPrompts`| `object` (optional) | A map from an `effectiveModel` ID to the system prompt string used for it. |
| `promptIds` | `string[]` | An array of all prompt IDs evaluated. |
| `promptContexts` | `object` | A map from `promptId` to the full input prompt, which can be a single string or an array of `ConversationMessage` objects for multi-turn conversations. |
| `allFinalAssistantResponses` | `object` (optional) | A nested map: `promptId` -> `modelId` -> `final assistant response text`. |
| `fullConversationHistories` | `object` (optional) | A nested map: `promptId` -> `modelId` -> `ConversationMessage[]` representing the full exchange. |
| `errors` | `object` (optional) | A nested map for any errors encountered: `promptId` -> `modelId` -> `error message`. |
| `extractedKeyPoints`| `Record<string, string[]>` (optional) | A map from `promptId` to the key points extracted by the judge model from the `idealResponse`. |
| `evaluationResults`| `object` | An object containing the results from all evaluation methods. See [Evaluation Results Structure](#2-evaluation-results-structure). |

### `ConversationMessage` Object

This object is used in `promptContexts` and `fullConversationHistories`.

```typescript
interface ConversationMessage {
  role: 'user' | 'assistant' | 'system';
  content: string;
}
```

---

## 2. Evaluation Results Structure

The `evaluationResults` object contains the outputs from the different evaluation methods.

| Key | Type | Description |
| --- | --- | --- |
| `similarityMatrix` | `SimilarityScore` (optional) | A matrix of pairwise semantic similarity scores, averaged across all prompts. Only present if `embedding` method is used. |
| `perPromptSimilarities` | `Record<string, SimilarityScore>` (optional) | A map from `promptId` to its individual `SimilarityScore` matrix. |
| `llmCoverageScores` | `LLMCoverageScores` (optional) | Contains rubric-based evaluation scores. Only present if `llm-coverage` method is used. |

### `SimilarityScore` Object

This is a nested object representing a pairwise similarity matrix.

```typescript
// Example for { modelA, modelB, modelC }
{
  "modelA": { "modelB": 0.85, "modelC": 0.62 },
  "modelB": { "modelA": 0.85, "modelC": 0.71 },
  "modelC": { "modelA": 0.62, "modelB": 0.71 }
}
```

### `LLMCoverageScores` Object

This is a nested map containing the results of the rubric-based (`llm-coverage`) evaluation.
`LLMCoverageScores`: `promptId` -> `modelId` -> `CoverageResult`

A `CoverageResult` object can either be a successful result or an error object.

#### Successful `CoverageResult`

```typescript
{
  "keyPointsCount": number, // Total number of rubric items for this prompt
  "avgCoverageExtent": number, // The final score (0 to 1), averaged across all points, respecting multipliers
  "pointAssessments": PointAssessment[] // An array of assessments, one for each rubric item
}
```

#### `PointAssessment` Object

This object contains the detailed evaluation for a single rubric item.

```typescript
{
  "keyPointText": string, // The original text of the rubric item
  "coverageExtent": number, // The score for this specific point (0 to 1)
  "reflection": string, // The judge model's reasoning for its score
  "error": string, // Present if an error occurred for this point
  "multiplier": number, // The weight of this point
  "citation": string, // Optional citation associated with the point
  "judgeModelId": string, // The identifier for the judge(s). For a consensus, this will be like `consensus(judge1(model), judge2(model))`.
  "isInverted": boolean, // True if the point came from a `should_not` block
  "pathId": string, // (Optional) An identifier present if the point belongs to an alternative path (OR logic) block. All points in the same path share a `pathId`.
  "judgeLog": string[], // (Debug) Log of steps taken by the judge logic
  "individualJudgements": IndividualJudgement[] // (Debug) The raw outputs from each judge in a consensus call
}
```

### `IndividualJudgement` Object

This object contains the specific output from a single judge within a consensus evaluation.

```typescript
{
  "judgeModelId": string, // The ID of the specific judge, e.g., `holistic(openai:gpt-4o-mini)`
  "coverageExtent": number,
  "reflection": string
}
```

---

## 3. Artefact-Based Run Layout (live/blueprints)

Each run produces a bundle of artefacts under `live/blueprints/[configId]/[runLabel]_[timestamp]/`:

- `core.json` – lightweight metadata and placeholders for bulky fields
- `responses/[promptId].json` – final assistant responses per prompt
- `coverage/[promptId]/[modelId].json` – per-prompt × model rubric evaluations
- `histories/[promptId]/[modelId].json` – full conversation history for this prompt×model, including generated assistant turns when `assistant: null` is used and the implicit final assistant if the last message is a user. Shape:

```json
{
  "history": [
    { "role": "user", "content": "I need help with my taxes." },
    { "role": "assistant", "content": "Sure—can you share your filing status and states involved?" },
    { "role": "user", "content": "I changed jobs mid-year and moved states." },
    { "role": "assistant", "content": "Thanks! Did you have income in both states?" },
    { "role": "user", "content": "Anything else I should consider?" },
    { "role": "assistant", "content": "Consider part-year residency rules, withholding, and credits..." }
  ]
}
```

## 4. Legacy Monolithic File

For backward compatibility, the original monolithic file `[runLabel]_[timestamp]_comparison.json` is still written in the same folder.

Sandbox runs additionally write a compatibility copy under:
- `live/blueprints/sandbox-<runId>/sandbox-run_<timestamp>_comparison.json`

This allows standard comparison endpoints to work in Sandbox result pages.