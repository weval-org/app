/**
 * Tests for PR Evaluation Limiter
 *
 * CRITICAL: These tests validate cost controls for PR evaluations.
 * A bug here could cost hundreds of dollars per PR!
 */

import {
  PR_EVAL_LIMITS,
  checkPREvalLimits,
  applyPREvalLimits,
  formatLimitViolations,
} from '../pr-eval-limiter';
import { ComparisonConfig } from '@/cli/types/cli_types';

// Mock axios for model collection fetching
jest.mock('axios');
import axios from 'axios';
const mockedAxios = axios as jest.Mocked<typeof axios>;

describe('PR_EVAL_LIMITS constants', () => {
  it('should have reasonable hardcoded limits', () => {
    expect(PR_EVAL_LIMITS.maxPrompts).toBe(10);
    expect(PR_EVAL_LIMITS.maxModels).toBe(5);
    expect(PR_EVAL_LIMITS.maxTemperatures).toBe(2);
    expect(PR_EVAL_LIMITS.maxSystemPrompts).toBe(2);
    expect(PR_EVAL_LIMITS.maxTotalResponses).toBe(100);
    expect(PR_EVAL_LIMITS.allowedModelCollections).toEqual(['CORE']);
  });
});

describe('checkPREvalLimits', () => {
  beforeEach(() => {
    jest.clearAllMocks();

    // Mock CORE collection to return 5 models
    mockedAxios.get.mockResolvedValue({
      data: ['openai:gpt-4', 'anthropic:claude-3-opus', 'openai:gpt-3.5-turbo', 'anthropic:claude-3-sonnet', 'google:gemini-pro']
    });
  });

  describe('prompts limit', () => {
    it('should allow blueprints within prompt limit', async () => {
      const config: ComparisonConfig = {
        id: 'test',
        prompts: Array(5).fill({ user: 'test prompt' }),
        models: ['openai:gpt-4'],
      };

      const result = await checkPREvalLimits(config);

      expect(result.allowed).toBe(true);
      expect(result.violations).toHaveLength(0);
    });

    it('should detect prompts exceeding limit', async () => {
      const config: ComparisonConfig = {
        id: 'test',
        prompts: Array(20).fill({ user: 'test prompt' }),
        models: ['openai:gpt-4'],
      };

      const result = await checkPREvalLimits(config);

      expect(result.allowed).toBe(false);
      expect(result.violations).toContainEqual(
        expect.objectContaining({
          limit: 'prompts',
          current: 20,
          max: 10,
        })
      );
    });

    it('should handle exactly at limit', async () => {
      const config: ComparisonConfig = {
        id: 'test',
        prompts: Array(10).fill({ user: 'test prompt' }),
        models: ['openai:gpt-4'],
      };

      const result = await checkPREvalLimits(config);

      expect(result.allowed).toBe(true);
    });
  });

  describe('temperature limit', () => {
    it('should allow blueprints within temperature limit', async () => {
      const config: ComparisonConfig = {
        id: 'test',
        prompts: [{ user: 'test' }],
        models: ['openai:gpt-4'],
        temperatures: [0.7],
      };

      const result = await checkPREvalLimits(config);

      expect(result.allowed).toBe(true);
    });

    it('should detect temperatures exceeding limit', async () => {
      const config: ComparisonConfig = {
        id: 'test',
        prompts: [{ user: 'test' }],
        models: ['openai:gpt-4'],
        temperatures: [0.3, 0.5, 0.7, 1.0, 1.5],
      };

      const result = await checkPREvalLimits(config);

      expect(result.allowed).toBe(false);
      expect(result.violations).toContainEqual(
        expect.objectContaining({
          limit: 'temperatures',
          current: 5,
          max: 2,
        })
      );
    });

    it('should handle undefined temperatures as single value', async () => {
      const config: ComparisonConfig = {
        id: 'test',
        prompts: [{ user: 'test' }],
        models: ['openai:gpt-4'],
        // temperatures undefined
      };

      const result = await checkPREvalLimits(config);

      expect(result.allowed).toBe(true);
    });
  });

  describe('system prompts limit', () => {
    it('should allow blueprints within system prompt limit', async () => {
      const config: ComparisonConfig = {
        id: 'test',
        prompts: [{ user: 'test' }],
        models: ['openai:gpt-4'],
        systems: ['system prompt 1'],
      };

      const result = await checkPREvalLimits(config);

      expect(result.allowed).toBe(true);
    });

    it('should detect system prompts exceeding limit', async () => {
      const config: ComparisonConfig = {
        id: 'test',
        prompts: [{ user: 'test' }],
        models: ['openai:gpt-4'],
        systems: ['sys1', 'sys2', 'sys3', 'sys4'],
      };

      const result = await checkPREvalLimits(config);

      expect(result.allowed).toBe(false);
      expect(result.violations).toContainEqual(
        expect.objectContaining({
          limit: 'systems',
          current: 4,
          max: 2,
        })
      );
    });

    it('should handle single system field', async () => {
      const config: ComparisonConfig = {
        id: 'test',
        prompts: [{ user: 'test' }],
        models: ['openai:gpt-4'],
        system: 'single system prompt',
      };

      const result = await checkPREvalLimits(config);

      expect(result.allowed).toBe(true);
    });
  });

  describe('model collection limits', () => {
    it('should allow CORE collection', async () => {
      const config: ComparisonConfig = {
        id: 'test',
        prompts: [{ user: 'test' }],
        models: ['CORE'],
      };

      const result = await checkPREvalLimits(config);

      expect(result.allowed).toBe(true);
      expect(mockedAxios.get).toHaveBeenCalledWith(
        expect.stringContaining('CORE.json'),
        expect.any(Object)
      );
    });

    it('should detect disallowed model collections', async () => {
      const config: ComparisonConfig = {
        id: 'test',
        prompts: [{ user: 'test' }],
        models: ['PREMIUM', 'EXPENSIVE'],
      };

      const result = await checkPREvalLimits(config);

      expect(result.allowed).toBe(false);
      expect(result.violations).toContainEqual(
        expect.objectContaining({
          limit: 'model_collections',
          message: expect.stringContaining('PREMIUM'),
        })
      );
      expect(result.violations).toContainEqual(
        expect.objectContaining({
          limit: 'model_collections',
          message: expect.stringContaining('EXPENSIVE'),
        })
      );
    });

    it('should allow direct model IDs', async () => {
      const config: ComparisonConfig = {
        id: 'test',
        prompts: [{ user: 'test' }],
        models: ['openai:gpt-4', 'anthropic:claude-3-opus'],
      };

      const result = await checkPREvalLimits(config);

      expect(result.allowed).toBe(true);
      // Should not try to fetch collections for direct model IDs
      expect(mockedAxios.get).not.toHaveBeenCalled();
    });

    it('should allow custom model definitions', async () => {
      const config: ComparisonConfig = {
        id: 'test',
        prompts: [{ user: 'test' }],
        models: [
          {
            id: 'custom:my-model',
            provider: 'openai',
            model: 'gpt-4',
          } as any,
        ],
      };

      const result = await checkPREvalLimits(config);

      expect(result.allowed).toBe(true);
    });

    it('should detect too many models after collection resolution', async () => {
      // Mock CORE to return more than maxModels
      mockedAxios.get.mockResolvedValueOnce({
        data: Array(10).fill(null).map((_, i) => `model-${i}`),
      });

      const config: ComparisonConfig = {
        id: 'test',
        prompts: [{ user: 'test' }],
        models: ['CORE'],
      };

      const result = await checkPREvalLimits(config);

      expect(result.allowed).toBe(false);
      expect(result.violations).toContainEqual(
        expect.objectContaining({
          limit: 'models',
          current: 10,
          max: 5,
        })
      );
    });
  });

  describe('total responses calculation', () => {
    it('should calculate total responses correctly', async () => {
      const config: ComparisonConfig = {
        id: 'test',
        prompts: Array(10).fill({ user: 'test' }),
        models: ['openai:gpt-4', 'anthropic:claude-3-opus'],
        temperatures: [0.5, 0.7],
        systems: ['sys1', 'sys2'],
      };

      const result = await checkPREvalLimits(config);

      // 10 prompts × 2 models × 2 temps × 2 systems = 80 responses
      expect(result.estimatedResponses).toBe(80);
      expect(result.allowed).toBe(true);
    });

    it('should detect total responses exceeding limit', async () => {
      mockedAxios.get.mockResolvedValueOnce({
        data: Array(5).fill(null).map((_, i) => `model-${i}`),
      });

      const config: ComparisonConfig = {
        id: 'test',
        prompts: Array(10).fill({ user: 'test' }),
        models: ['CORE'], // resolves to 5 models
        temperatures: [0.5, 0.7], // 2 temps
        systems: ['sys1', 'sys2'], // 2 systems
      };

      const result = await checkPREvalLimits(config);

      // 10 × 5 × 2 × 2 = 200 > 100
      expect(result.estimatedResponses).toBe(200);
      expect(result.allowed).toBe(false);
      expect(result.violations).toContainEqual(
        expect.objectContaining({
          limit: 'total_responses',
          current: 200,
          max: 100,
        })
      );
    });

    it('should handle exactly at total response limit', async () => {
      const config: ComparisonConfig = {
        id: 'test',
        prompts: Array(10).fill({ user: 'test' }),
        models: ['openai:gpt-4', 'anthropic:claude-3-opus'], // 2 models
        temperatures: [0.7], // 1 temp
        systems: ['sys1', 'sys2', 'sys3', 'sys4', 'sys5'], // 5 systems
      };

      const result = await checkPREvalLimits(config);

      // 10 × 2 × 1 × 5 = 100 (exactly at limit)
      expect(result.estimatedResponses).toBe(100);
      expect(result.allowed).toBe(false); // systems exceed limit (5 > 2)
    });
  });

  describe('multiple violations', () => {
    it('should report all violations', async () => {
      mockedAxios.get.mockResolvedValueOnce({
        data: Array(10).fill(null).map((_, i) => `model-${i}`),
      });

      const config: ComparisonConfig = {
        id: 'test',
        prompts: Array(50).fill({ user: 'test' }),
        models: ['CORE', 'PREMIUM'], // PREMIUM not allowed, CORE has too many
        temperatures: [0.1, 0.3, 0.5, 0.7, 1.0],
        systems: ['sys1', 'sys2', 'sys3'],
      };

      const result = await checkPREvalLimits(config);

      expect(result.allowed).toBe(false);
      expect(result.violations.length).toBeGreaterThan(1);

      // Check we have violations for each dimension
      const violationLimits = result.violations.map(v => v.limit);
      expect(violationLimits).toContain('prompts');
      expect(violationLimits).toContain('models');
      expect(violationLimits).toContain('temperatures');
      expect(violationLimits).toContain('systems');
      expect(violationLimits).toContain('model_collections');
    });
  });
});

describe('applyPREvalLimits', () => {
  beforeEach(() => {
    jest.clearAllMocks();

    // Mock CORE collection
    mockedAxios.get.mockResolvedValue({
      data: ['openai:gpt-4', 'anthropic:claude-3-opus', 'openai:gpt-3.5-turbo', 'anthropic:claude-3-sonnet', 'google:gemini-pro']
    });
  });

  it('should trim prompts to maxPrompts', async () => {
    const config: ComparisonConfig = {
      id: 'test',
      prompts: Array(50).fill({ user: 'test prompt' }),
      models: ['openai:gpt-4'],
    };

    const trimmed = await applyPREvalLimits(config);

    expect(trimmed.prompts?.length).toBe(10);
    // Should keep first 10
    expect(trimmed.prompts).toEqual(config.prompts?.slice(0, 10));
  });

  it('should not modify prompts within limit', async () => {
    const config: ComparisonConfig = {
      id: 'test',
      prompts: Array(5).fill({ user: 'test prompt' }),
      models: ['openai:gpt-4'],
    };

    const trimmed = await applyPREvalLimits(config);

    expect(trimmed.prompts?.length).toBe(5);
    expect(trimmed.prompts).toEqual(config.prompts);
  });

  it('should trim temperatures to maxTemperatures', async () => {
    const config: ComparisonConfig = {
      id: 'test',
      prompts: [{ user: 'test' }],
      models: ['openai:gpt-4'],
      temperatures: [0.1, 0.3, 0.5, 0.7, 1.0, 1.5],
    };

    const trimmed = await applyPREvalLimits(config);

    expect(trimmed.temperatures?.length).toBe(2);
    expect(trimmed.temperatures).toEqual([0.1, 0.3]);
  });

  it('should trim systems to maxSystemPrompts', async () => {
    const config: ComparisonConfig = {
      id: 'test',
      prompts: [{ user: 'test' }],
      models: ['openai:gpt-4'],
      systems: ['sys1', 'sys2', 'sys3', 'sys4', 'sys5'],
    };

    const trimmed = await applyPREvalLimits(config);

    expect(trimmed.systems?.length).toBe(2);
    expect(trimmed.systems).toEqual(['sys1', 'sys2']);
  });

  it('should filter to allowed collections and limit count', async () => {
    mockedAxios.get.mockImplementation((url: string) => {
      if (url.includes('CORE.json')) {
        return Promise.resolve({
          data: Array(10).fill(null).map((_, i) => `core-model-${i}`)
        });
      }
      // PREMIUM not fetched (not allowed)
      return Promise.reject(new Error('Not found'));
    });

    const config: ComparisonConfig = {
      id: 'test',
      prompts: [{ user: 'test' }],
      models: ['CORE', 'PREMIUM'], // PREMIUM should be filtered out
    };

    const trimmed = await applyPREvalLimits(config);

    // Should only have CORE models, limited to maxModels
    expect(trimmed.models?.length).toBeLessThanOrEqual(5);

    // Verify no PREMIUM models
    const modelStrings = trimmed.models?.filter(m => typeof m === 'string') as string[];
    expect(modelStrings.every(m => !m.includes('premium'))).toBe(true);
  });

  it('should preserve custom model definitions', async () => {
    const customModel = {
      id: 'custom:my-model',
      provider: 'openai',
      model: 'gpt-4',
    };

    const config: ComparisonConfig = {
      id: 'test',
      prompts: [{ user: 'test' }],
      models: [customModel as any, 'openai:gpt-4'],
    };

    const trimmed = await applyPREvalLimits(config);

    expect(trimmed.models).toContain('custom:my-model');
  });

  it('should not mutate original config', async () => {
    const config: ComparisonConfig = {
      id: 'test',
      prompts: Array(50).fill({ user: 'test' }),
      models: ['CORE'],
      temperatures: [0.1, 0.3, 0.5, 0.7, 1.0],
      systems: ['sys1', 'sys2', 'sys3'],
    };

    const originalPromptCount = config.prompts?.length;
    const originalTempCount = config.temperatures?.length;
    const originalSystemCount = config.systems?.length;

    await applyPREvalLimits(config);

    expect(config.prompts?.length).toBe(originalPromptCount);
    expect(config.temperatures?.length).toBe(originalTempCount);
    expect(config.systems?.length).toBe(originalSystemCount);
  });

  it('should apply all limits together', async () => {
    mockedAxios.get.mockResolvedValueOnce({
      data: Array(10).fill(null).map((_, i) => `model-${i}`)
    });

    const config: ComparisonConfig = {
      id: 'test',
      prompts: Array(50).fill({ user: 'test' }),
      models: ['CORE'],
      temperatures: [0.1, 0.3, 0.5, 0.7, 1.0],
      systems: ['sys1', 'sys2', 'sys3', 'sys4'],
    };

    const trimmed = await applyPREvalLimits(config);

    expect(trimmed.prompts?.length).toBe(10);
    expect(trimmed.models?.length).toBe(5);
    expect(trimmed.temperatures?.length).toBe(2);
    expect(trimmed.systems?.length).toBe(2);

    // Verify total responses now within limit
    const totalResponses = 10 * 5 * 2 * 2; // = 200
    // This still exceeds 100, so in practice the trimming logic might need adjustment
    // or we check violations again after trimming
  });
});

describe('formatLimitViolations', () => {
  it('should format violations into user-friendly message', () => {
    const violations = [
      {
        limit: 'prompts',
        current: 50,
        max: 10,
        message: 'Blueprint has 50 prompts, but PR evaluations are limited to 10',
      },
      {
        limit: 'models',
        current: 15,
        max: 5,
        message: 'Blueprint resolves to 15 models, but PR evaluations are limited to 5',
      },
    ];

    const message = formatLimitViolations(violations);

    expect(message).toContain('Blueprint exceeds PR evaluation limits');
    expect(message).toContain('50 prompts');
    expect(message).toContain('15 models');
    expect(message).toContain('Max prompts: 10');
    expect(message).toContain('Max models: 5');
    expect(message).toContain('automatically trimmed');
  });

  it('should include all limit values', () => {
    const violations = [
      {
        limit: 'prompts',
        current: 20,
        max: 10,
        message: 'Test violation',
      },
    ];

    const message = formatLimitViolations(violations);

    expect(message).toContain(`Max prompts: ${PR_EVAL_LIMITS.maxPrompts}`);
    expect(message).toContain(`Max models: ${PR_EVAL_LIMITS.maxModels}`);
    expect(message).toContain(`Max temperatures: ${PR_EVAL_LIMITS.maxTemperatures}`);
    expect(message).toContain(`Max system prompts: ${PR_EVAL_LIMITS.maxSystemPrompts}`);
    expect(message).toContain(`Max total responses: ${PR_EVAL_LIMITS.maxTotalResponses}`);
    expect(message).toContain(`Allowed model collections: ${PR_EVAL_LIMITS.allowedModelCollections.join(', ')}`);
  });
});
