/**
 * This file contains the experimental types for the Model Card generation feature.
 * They are kept separate from `src/types/shared.ts` during development.
 */

/**
 * Represents the performance of a model in a single evaluation run.
 * This is a piece of the "evidence" that gets stored in a ModelSummary.
 */
export interface ModelRunPerformance {
  configId: string;
  configTitle: string;
  runLabel: string;
  timestamp: string;
  hybridScore: number;

  // -- Critical Comparative Context --
  peerAverageScore: number | null; // The average score of all other models in this run.
  rank: number | null;             // The model's rank in this run (e.g., 1st, 2nd, 10th).
  totalModelsInRun?: number; // Total number of models that participated in this specific run

  // -- Critical Qualitative Context --
  executiveSummary: string | null; // The exec summary from this run, which is rich with comparative insight.
}

/**
 * Represents a top performing evaluation for display purposes.
 */
export interface TopPerformingEvaluation {
  configId: string;
  configTitle: string;
  runLabel: string;
  timestamp: string;
  hybridScore: number;
  rank: number | null;
  totalModelsInRun: number;
  relativePeerAdvantage: number | null; // How much better than peer average (hybridScore - peerAverageScore)
  analysisUrl: string; // Direct link to the analysis page
}

/**
 * The main data structure for a "Model Card".
 * Contains both hard aggregated data and synthesized qualitative analysis.
 */
export interface ModelSummary {
  modelId: string;        // The pattern used to generate this card (e.g., "haiku", "gpt-4o")
  displayName: string;
  provider: string;
  discoveredModelIds?: string[]; // All actual model IDs that matched the pattern (e.g., ["claude-3-haiku", "claude-3-5-haiku"])
  systemPromptMappings?: Record<string, Record<string, string>>; // configId -> effectiveModelId -> actual system prompt text
  
  // --- Part 1: Hard Data Aggregates ---
  overallStats: {
    averageHybridScore: number | null;
    totalRuns: number;
    totalBlueprints: number;
    runs: ModelRunPerformance[]; // The full "evidence locker" of every run.
  };

  // Performance broken down by blueprint tags, e.g., "Safety", "Code-Gen"
  performanceByTag?: {
    [tag: string]: {
      averageScore: number | null;
      blueprintCount: number;
    };
  };

  // Top performing evaluations - sorted by performance
  topPerformingEvaluations?: TopPerformingEvaluation[];

  // Aggregated dimensional grades across all evaluations
  dimensionalGrades?: {
    [dimensionKey: string]: {
      averageScore: number;
      evaluationCount: number;
      label: string;
    };
  };

  // --- Part 2: Synthesized Analytical Insights ---
  // This entire object is generated by the "Meta-Analyst" LLM
  analyticalSummary?: {
    narrative: string;   // The full, readable text with embedded XML-like tags.
    strengths: string[]; // Parsed from <strength> tags.
    weaknesses: string[];// Parsed from <weakness> tags.
    risks: string[];     // Parsed from <risk> tags.
    patterns: string[];  // Parsed from <pattern> tags.
    lastUpdated: string; // Timestamp of when this analysis was last generated.
  };

  lastUpdated: string; // Timestamp of when this summary was last updated with new run data.
} 