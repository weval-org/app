"""GitHub PR creation for benchmark Blueprints."""

from __future__ import annotations

import logging
import shutil
import subprocess
from pathlib import Path

from .models import PaperAnalysis, PipelineConfig, TrackingEntry

logger = logging.getLogger(__name__)


def create_blueprint_pr(
    blueprint_path: Path,
    entry: TrackingEntry,
    analysis: PaperAnalysis,
    config: PipelineConfig,
) -> str | None:
    """Create a PR to weval-org/configs with the Blueprint.

    Returns the PR URL, or None on failure.
    """
    configs_path = Path(config.configs_local_path)
    if not configs_path.exists():
        logger.error(f"Configs repo not found at: {configs_path}")
        return None

    # Verify gh CLI is available
    if not shutil.which("gh"):
        logger.error(
            "GitHub CLI (gh) not found. Install it: https://cli.github.com/"
        )
        return None

    branch_name = f"{config.branch_prefix}/{blueprint_path.stem}"

    try:
        # Ensure we're on main and up to date
        _run_git(["checkout", "main"], configs_path)
        _run_git(["pull", "origin", "main"], configs_path)

        # Create new branch
        _run_git(["checkout", "-b", branch_name], configs_path)

        # Copy Blueprint to blueprints directory
        dest = configs_path / "blueprints" / blueprint_path.name
        shutil.copy2(blueprint_path, dest)
        logger.info(f"Copied Blueprint to {dest}")

        # Stage and commit
        _run_git(["add", str(dest)], configs_path)
        commit_msg = f"Add {analysis.benchmark_name or entry.title} benchmark blueprint"
        _run_git(["commit", "-m", commit_msg], configs_path)

        # Push
        _run_git(["push", "-u", "origin", branch_name], configs_path)

        # Create PR
        pr_body = _format_pr_body(entry, analysis)
        pr_title = f"Add {analysis.benchmark_name or entry.title} benchmark"
        if len(pr_title) > 70:
            pr_title = pr_title[:67] + "..."

        result = subprocess.run(
            [
                "gh",
                "pr",
                "create",
                "--title",
                pr_title,
                "--body",
                pr_body,
                "--repo",
                config.configs_repo,
            ],
            cwd=configs_path,
            capture_output=True,
            text=True,
            check=True,
        )

        pr_url = result.stdout.strip()
        logger.info(f"Created PR: {pr_url}")

        # Switch back to main
        _run_git(["checkout", "main"], configs_path)

        return pr_url

    except subprocess.CalledProcessError as e:
        logger.error(f"Git/gh command failed: {e.stderr}")
        # Try to recover to main
        try:
            _run_git(["checkout", "main"], configs_path)
        except Exception:
            pass
        return None
    except Exception as e:
        logger.error(f"PR creation failed: {e}")
        try:
            _run_git(["checkout", "main"], configs_path)
        except Exception:
            pass
        return None


def _run_git(args: list[str], cwd: Path) -> subprocess.CompletedProcess:
    """Run a git command."""
    cmd = ["git"] + args
    logger.debug(f"Running: {' '.join(cmd)}")
    return subprocess.run(cmd, cwd=cwd, capture_output=True, text=True, check=True)


def _format_pr_body(entry: TrackingEntry, analysis: PaperAnalysis) -> str:
    """Format the PR description."""
    authors = ", ".join(entry.authors) if entry.authors else "Unknown"
    prompt_count = len(analysis.prompts)

    return f"""## Summary

Automated benchmark Blueprint generated from academic paper.

- **Paper:** [{entry.title}]({entry.arxiv_url})
- **Authors:** {authors}
- **Benchmark:** {analysis.benchmark_name}
- **Evaluation type:** {analysis.evaluation_type}
- **Prompts:** {prompt_count} evaluation prompts extracted
- **Metrics:** {', '.join(analysis.metrics) if analysis.metrics else 'N/A'}

## Description

{analysis.description}

## Source

This Blueprint was generated by the Weval Benchmark Pipeline from the paper's
published evaluation methodology. Please review the extracted prompts and
scoring criteria for accuracy.

## Test plan

- [ ] Verify Blueprint YAML is valid
- [ ] Review extracted prompts match paper methodology
- [ ] Confirm scoring criteria are appropriate
- [ ] Run evaluation against CORE models
"""
